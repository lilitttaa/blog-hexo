---
title: Games001 13.Solution of Linear Systems（线性系统求解）
mathjax: true
date: 2025-01-07 00:00:00
category:
 - Mathematics
sortValue: 130003
---

## Introduction

线性系统要解的问题就是，求：$$Ax = b$$这个矩阵方程组的解

图形学中矩阵方程出现在很多地方：
![alt text](image.png)

- 几何处理中隐式的 Laplace Smoothing
- 物理模拟中每一个时间步内都需要解一个大的线性方程组
- 全局渲染中的 radiosity-based method
- 如何针对不同的问题选择合适的矩阵求解器，并且让它解的又稳定又快又好是这里的中心问题

## 直接求解

### LU Decomposition

最直接的方法是使用高斯消元直接求解：
![alt text](image-1.png)
![alt text](image-2.png)

- 先求得这么一个上三角矩阵

![alt text](image-3.png)

- 通过反向的高斯消元法，求解出最终的解

上面的每一步行变换我们都可以表述成矩阵运算的形式：
![alt text](image-4.png)
所有的行变换构成一个 L 矩阵（下三角矩阵），而求得的矩阵就是 U 矩阵（上三角矩阵）：
![alt text](image-5.png)

- 高斯消元法把$Ax=b$分解成了$LUx=b$，然后我们可以先把 Ux 看作一个未知向量 y 求解$Ly=b$，然后通过$Ux=y$求解 x

![alt text](image-7.png)

- 这个算法是 Inplace 的，L 矩阵和 U 矩阵都存在原本的 A 矩阵里边，其中 L 矩阵的对角线都是 1，所以存储的时候直接忽略了，左下角部分存 L，右上角部分存 U。

![alt text](image-6.png)

- 比如第一行消第二行不光把第一个元素消掉了，还把第二个元素消掉了，这样后面做除法的时候就会出现除以 0，会导致数值不稳定。

### Cholesky Decomposition

![alt text](image-8.png)

- LU 分解在对称半正定的矩阵中有更简单的算法，这也是一种非常常用的算法，叫做 Cholesky Decomposition，也称之为 LLT Decomposition
- 在 A 是对称半正定矩阵时下三角矩阵和上三角矩阵是转置的关系

![alt text](image-9.png)

- 从$L_{11}$开始一直往下走，最后得到整个 L 矩阵的各个系数
- 计算量可以比 LU 分解减小一半，不过还是 O(n^3)的复杂度

![alt text](image-10.png)

![alt text](image-11.png)

- 前面提到的这两种算法一般来说只适用于求解小的矩阵，一般 n 小于 1000
- 但是图形学通常会有非常大的矩阵，这种方法可能需要花几十分钟甚至几个小时来解决

## 迭代求解

![alt text](image-12.png)

- 对于大规模的矩阵方程，一般使用迭代求解的方法
- 对于矩阵 A 是个黑盒但是 Ax 已知的情况也适用

![alt text](image-13.png)

- 这个弹性模型有 20 w 个节点，对应的矩阵方程的自由度是 60 w 维的矩阵

### Stationary Iteration（不动点迭代）

![alt text](image-14.png)

- 不断把$cos(x_i)$带入进去计算，最后会收敛到一个特定的值，这个值就是$x = cos(x)$的解

![alt text](image-15.png)

- 不动点迭代就是构造一个两项相减的形式
- 对于矩阵来说，我们把 A 分解为 M 和 N 相减

### Jacobi Iteration（雅可比迭代）

![alt text](image-16.png)
![alt text](image-17.png)

Jacobi Iteration 有时候会收敛，有时候会发散：
![alt text](image-18.png)

- 对于 3x3 的矩阵，Jacobi Iteration 需要十几步才能收敛

![alt text](image-19.png)

那么具体是哪些情况会失败呢？

### 不动点迭代的收敛条件

$$x^{k+1} = M^{-1}(Nx^k+b)$$
我们令 $N = M - A$，那么：$$x^{k+1} = x^k + M^{-1}(b - Ax^k)$$
定义误差：$$e_k = x^* - x^k$$，其中 $x^*$ 是真实解，那么残差：$$r_k = b - Ax^k = Ax^* - Ax^k = Ae_k$$
用$Ae_k$替换$b - Ax^k$，那么：$$x^{k+1} \leftarrow x^k + M^{-1}Ae_k$$
则：$$x^* - x^{k+1} \leftarrow x^* - x^k - M^{-1}Ae_k$$
所以误差更新关系是：
$$e_{k+1} \leftarrow (I - M^{-1}A)e_k = M^{-1}N e_k$$

![Alt text](image-20.png)

- 里边的特征值越小，收敛越快
- 关于特征值分解可以参考：https://zhuanlan.zhihu.com/p/314464267

![Alt text](image-21.png)

- 前面的标准在实际应用中使用比较困难，因为我们不太可能先求出 T 矩阵再对它进行特征值分解来判断每个特征值是否小于 1。所以实际应用中会使用一个简单的判断标准。
- 简单来说就是每行对角元素大于非对角元素的绝对值之和。

![Alt text](image-22.png)

- 对于非对角占优矩阵，我们可以使用松弛的方法，每次更新的步长乘以一个常数 $\omega$
- 如果矩阵不收敛，要取一个尽量小的值

![Alt text](image-23.png)

### Gauss-Seidel Iteration（高斯赛德尔迭代）

![Alt text](image-24.png)

- 其中$M^{-1}b$的计算可以利用高斯消元法中计算$Mx=b$的方式计算出$x$，这个$x$也就是$M^{-1}b$
- Jacobi M 是对角矩阵，我们可以并行的求逆。但 Gauss-Seidel M 是下三角矩阵，我们只能串行的求逆。不过可以通过重排矩阵的方式得到并行版本的 Gauss-Seidel 矩阵

![Alt text](image-25.png)

### Conclusion

![Alt text](image-26.png)

- 关于 MultiGrid Method，可以参考：[A Multigrid Tutorial, 2nd Edition](https://www.math.hkust.edu.hk/~mamu/courses/531/tutorial_with_corrections.pdf)

## Subspace Methods（子空间方法）

![Alt text](image-27.png)

OK 那我们到目前为止我们介绍了两种

所以矩阵方程的方法对吧

第一种是这个直接求解方法

就是这个 LU 分解和 LOT 分解

然后还有不动点迭代方法

就是这个 joping integration 和这个高 seal iteration

那我们下面其实在介绍这个第三种呃

迭代求解指这个矩阵方程的方法

称之为这个子空间方法

这个子空间方法他的数学理论是比较复杂的

不是很直观

没有像这个不动点不动点迭代那样直观

我尽量用这个这个就是简单的语言来给大家

就是描述一下这个子空间迭代

它具体的这个思基本的思想是什么

那么出发点是什么呢

是我们可以把 X 等于 B 这样一个问题

看成是一个优化问题对吧

我们去优化这个 X 减 B 的这个残差

那么整个优化的空间呢是 RN 对吧

就是 A 是 N 维的那种优化空间

就是 RN 那么假设中间有一组基

这个 RN 上有一组积 E1 到 EN

那么子空间迭代的一个基本思想是什么呢

是跟我们在第 M 部 M 部里面

我们只考虑这个在子空间

由 E1 到 EM 章程的这个 M 为子空间上的

这样一个优化问题的解

也就是我们限制了 X

它只能由哎 E1 到 EM

这 M 个向量的线性组合构成

然后只考虑 X 在这个 MX 空间上

如何让这个 X 减 B 它的残差最小

那么通过这种方式呢

由于我们在就是因为我们限制到了我们的问题

在子空间上

所以我们在每一步迭代的时候

这个考虑的这个优化问题的这个维度

就不再是整个优化空间了

而是在一个子空间上考虑这个问题

那么并且如果我这个 GE1 到 EN

它的构造方式满足某些特性

那么满满足某些数值呃

这个数学上的特性的话

我们就可以用很少的这个计算

从 XM 减一更新到 XM

XM 减一是在 E1 到 EM 减一

这 M 减一个向量上的这个子空间上的解

XM 是一到 em 这个 M 位子空间上的解

如果这个基

那构造的方式满足某些数学特性的话

那么从 XM 减一更新到 XM

这个代价它其实是低的

所以额那么那么这样的话

我这个子空间迭代方法就可以进行下去对吧

因为我每次迭代的每每一步里面

这个代价是低的

然后同时子空间迭代保证了说

我们最多只需要 N 步就一定可以达到最终的解

为什么

因为在第 N 步的时候

我们保证我们求解的这个解

是以一一和到 EN 这 N 个向量章程的 N 维

子空间上的这个残差最小化的解

那么也就是原来的整个空间

RN 上的这个最小化的解

所以说我们最多只需要 N 步

就可以达到这个最终的这个解

那么相比之下

我们刚才说这个后面迭代和高赛道迭代

它是可能迭代很多很多步的对吧

我们刚才一个 3×3 的矩阵需要迭代十几步

但是如果我们使用子空间迭代的话

我们最多只需要迭代三步

就已经能够保证这个把解求出来

### 00:52:04,50

![00-52-04](tmp/00-52-04.jpg)

是吗

额

那我们下面就具体来看一看

这个子空间迭代到底是怎么进行的对吧

那么最常用的子空间迭代方法

或者最成功的子空间迭代方法

就是这个基于这个 clot 夫子空间的方法

那 clot 是一个嗯

应该是前苏联或者俄罗斯的这个数学家

那么由他提出的这个 CROODS 子空间

那么具体长什么样子呢

就是对于这个矩阵方程 X 等于 B 来说

quo 子空间定义成下面这个样子

KRAB 等于后面这一坨

这个数学含义是什么呢

是说整个子空间它是由 BAB

A 的 R1 直到 A 的 R 减一次方

乘以 B 这 R 个向量所作为啊

所章程的子空间

那么整个子空间

它就是一个 R 维的一个额子空间

因为我们这地方有 R 个向量

对吧

然后 BABA 到 R 四 R1 直到 A 到 R 减一次方

B 它中间这些 G 也不一定是啊

这些向量也不一定是正交的对吧

我们可以把这些向量啊作为基

但是这个基呢它就是一个不正交的一个基

所以 KR 是一个 R 维的子空间

那并且呢他满足这样一个嵌套关系

就是 K1 是这个 K2 的子空间

然后 K2 又是 K3 的子空间

一直这样嵌套下去

这个很显然对吧

因为 K2 相比 K1 来说的话

我就是多了 AB 这样一个机

然后 K3 相比 K2 来说的话

我就多了一个 A 的平方 B 这样的几

那么一直到这个 KN 也就是我会有 BA 的 B 次方

A 的 N 减一次方乘以 B 这 N 个向量

它所章程的空间就是全空间 RN 之后的

所有这个 KN 加一

KN 加二

虽然我的向量个数变多了

但是他这空间的维度是没有没有变多的常数

就这样

N 加 1N 加二个向量

它之间是有这样一个线性关系的

那么使用 quo 子空间进行

我们刚才说的这个子空间迭代方法

那么也就是说我们在 DM 部考虑的问题

就是在这个 km 中寻找这个最优化残差的

这个 XM

你这是这个就是关于这个 QUOS 子空间

迭代方法的一个最基本的描述

### 00:54:54,50

![00-54-54](tmp/00-54-54.jpg)

那我们下面就看一个具体的例子

也是最常用的这个 COU 子空间的迭代方法

就是共轭梯度下降方法

Contra gradient method

它所解决的问题是对于对称半正定的矩阵 A

我们要求解一个 X 等于 B 的问题

那么对于像这个拐点方法

或者说 cg 方法来说的话

我们把 X 等于 B 这样一个矩阵方程

转化成下面这样一个形式的一个优化问题

那么这个优化问题把取到最小值

取到最小值的时候

对应的就是它的梯度等于零对吧

那么梯度是什么呢

把这个后面这个形式求导

对于这个 X 求导

那么求出来结果就是 AX 减去 B

它刚好就是我们的负的残差对吧

那么如果上面这个形式它取到最小

那就是残差等于零

也就是 X 等于 B 了

然后并且由于 A 矩阵是对称半正定的

所以我能保证整个后面这个东西呢

它就是一个一个正的一个二次型对吧

所以他有唯一的这个最小值额

那因此呢我们解决这样一个最小化问题

就等价于解 X 等于 B

那么如果我们刚才说说这个呃 QUOS 空间方法

在 DN 步中

我们想要的是让 XM 在 km 中最小化前差

或者最小化这个 FX 对吧

这个是我们在子空间迭代中要做的

那么这个最小化什么时候能取到呢

这个就是些几何知识了

就是这个梯度啊

拉姆达 F 也就是负的 RM

这个梯度它如果正交于我们的子空间的话

那么这个时候我就能保证整个这个解

它是这个额在 km 中最小化了

Fx

这是个几何结论对吧

如果我的这个额

如果我的这个梯度正交于我的这个子空间

那代表我无论在怎么在这个子空间中移动

我都无法再进一步减小它的这个 FX 的值了

因为我要减小它的话

我一定是沿着这个负 RM 这个方向移动对吧

我要沿着这个负的梯度的方向去优化这个 FX

才能使它更小

但是由于你这个啊负 RM

跟我现在子空间是垂直的关系

所以我无论在这个子空间中怎么移动

都没有办法让它的这个残差进一步变小

所以说啊

所以说额那么我们在每一步迭代过程中

就需要保证这样一个条件

在每一步迭代当中

我们需要都需要让这个 RM 和当前的这个子空间

KM 是正交的

只要这满足了这个条件

我就能知道 XM 是在 cam 中最小化了 FX

那我的这个子空间迭代算法就成立了

### 00:58:16,50

![00-58-16](tmp/00-58-16.jpg)

OK 那么下面就看这个公和梯度方

公过梯度方法是如何去保证

我们上面说的这个 RM 前差垂直于 km

它是在每一步都能成立的

那么我们这地方给出一个共和

梯度下降算法的一个最核心的部分

它的核心思想是什么呢

是我们如果我们能够构造这个 QUOB 子空间

他有一系列的共轭的 GPM

满足下面这样的关系

就是 P1 到 PM 是 M 个向量

然后这 M 个向量张成的空间

就是我们刚才说的这个 high love 子空间

KMMMV 的子空间

并且这呃这 N 个向量 P 到呃

呃这这这所有的向量 P1

一直到 PM 一直到 PN

他们都满足这样一个共轭关系

也就是 pi 转置乘以 A 乘以 PJ 等于零

如果我们能够构造出 QUOS 空间的这一系列的

共同的基

那么

在迭代的每一步

我们在已知 XM 减一

求 YMXM 的过程

就是在 PM 的方向上求最小化 FX 的过程

写成数学形式就是啊已知 XM 减一

那么 XM 和 XXM 减一的关系

就是这样一个关系

这个阿尔法 M 是一个标量

gm 是一个啊

是一个我已知的这个啊向量的方向

那么我们现在要知道

就是我要去求解一个阿尔法 M

使得它能够最小化 FX

那由于这个阿尔法它是一个标量

那么这样一个优化问题

他就是个一维的优化问题

他就非常好解

那么这东西就是共轭梯度下降算法的核心

那么啊这样的过程就是大家先不去想说

就我们这个 P1 到 PM 到底是怎么来的

以及呃

以及这个地方为什么 XM 减一和 XM 之间

就是这样一个关系对吧

我们先把这个结论告诉大家

就是说

如果我们已经有了 P1 到 PM

这样一个一个一系列共和机的话

那么我在每一步迭代的时候

我只需要去解一个一维的优化问题

就可以得到从 X 减一更新到 XM

那这个东西就是我们啊

这个共和梯度下降算法的一个核心

那么我们下一步的问题就是说

为什么这样一个额这样一个算法

它就能够保证我们刚才说的这个性质

就是 XAM 它对应的那个梯度 RM

它是正交于 KM 的

我们下面就去证明这样一个啊这样一个过程

它是满足我们刚才的要求的

### 01:01:30,0

![01-01-30](tmp/01-01-30.jpg)

怎么证呢

我们就归纳了来证明

那么最开始在迭代最开始的时候

我们是在这个由 P1 构成了

这个额这一个一维的子空间上

去解这样一个优化问题对吧

那么 P1 显因为因为他是个一维的

所以那这个优化问题就可以直接解

然后并且解完了这个解完之后呢

我能够保证这个 R1 它是垂直于 K1 的对吧

因为我是在这个一位问题上解到了最优

所以这个 R1 它的梯度或者它的残差

是垂直于当前的子空间 K1 的

然后如果在 DM 减一部的时候

我们保证了 RM 减一是和 KM 减一是垂直的

那我们下面去考虑 RM 和 KM 的关系怎么考虑呢

我们把 RM 写出来对吧

RM 等于 B 减去 A 乘以 XN

然后 XM 呢用我们刚才引入的这样一个

更新的关系代入进去解开

然后呢 B 减去 A 乘以 XM 减一

这一项就是对应的 RM 减一

然后面一项这个阿尔法 M 乘以 A 乘以 pm

保留对吧

所以 RM 和 RN 减一满足这样一个关系

我们要证明什么

要证明 RM 和 KM 是垂直的

怎么证明呢

首先第一点

由于我是在 pm 这个方向上进行了搜索对吧

我是在 pm 方向上前面乘了这个一个 scale r

在这个方向上进行搜索

最小化了 FX 满足了最小化条件

因为我在这样一个一维优化问题上

满足最小化条件

那么一定有这个 RN 是垂直于 PM 的

因为这个 PM 就是额外增加的

那一维的那个方向对吧

如果我在这个方向上最小化了这个 FX

那么你剩下的残差

一定是跟这个方向垂直的对吧

所以这是第一个我们要用的结论

那第二个结论

由于我们啊由于 RN 减一垂直于 KN 减一

这是上一步我们的这个归纳的

这个用的这个条件

然后以及 PM 也垂直 KM 减一

因为这个条件呢就是 PMPI 和 PJ 任意一对

PIIPJ 它们之间是一个共轭的关系

所以 PM 也是垂直于 km 减一的

对吧

这个就是我们构造这个 G 的时候

用到它的共轭关系

那因此 gm p m 和 km 减一垂直

RM 减一也和 km 减一垂直

所以 RM 就自然和这个 KM 减一垂直

因为 RM 就是 RM 减一

再乘以这个后面这个东西

对吧嗯我看一下哦

不好意思

这地方应该是 APM 垂直于 km 减一

这边改一下

那算了

待会就改了

因为啊对

因为这个地方是因为呃对

pp 是 pi 和 PJ 中间有个 A 矩阵嘛对吧

所以这地方应该是 APM 垂直于 km 减一

OK 那这样的话结论应该是成立

就是 RM 是垂直于 KM 减一的

那因此呢我们证明了 RM 他是跟 KM 减一垂直的

并且呢 RM 也是跟 PM 垂直的

那自然啊 RM 就是和 km 垂直的

那 km 就是 km 减一和 PM 组成的

那换言之呢

我们通过这样的归纳的方法证明了说

只要我们能够构造出啊一切这个共轭机

P1 和 P 到 PN

那么 CD 算法它就是可以进行下去的

在每一步我们都保证了这个梯度

RM 是和当前的子空间垂直的

也就是说我们每一步都保证了这个呃

我们是在这个当前子空间取到了最优

然后并且呢在每一步更新的时候

我只我其实只需要考虑在 PM 这个方向上的

一个一维的优化问题

所以我的迭代就可以进行下去了

OK 这个就是工位梯度算法的一个基本的一个

### 01:05:54,0

![01-05-54](tmp/01-05-54.jpg)

一个想法

我们当然这地方还没有涉及到

说如何如何去构造这样一个 P 对吧

但我们今天就不展开讲了

直接把这个算法具体形式来告诉大家

那么这样的算法其实写出来会非常简单对吧

你可以看到说呃

前面一部分这个 R 求 R 发 K 的这样的过程

它其实就是在描述我们刚才说的这个呃

解在 pk 方向的一维优化问题的过程对吧

XK 减一等于 SK 加上阿尔法 K 乘以 PK

然后阿尔法 K 呢是一个我们需要去优化的东西

它优化的这个值就是后面这坨东西

然后拿到阿尔法 K 之后呢

我们下一步就去构造 pk 加一

作为下一步的这个搜索方向

这个搜索方向要跟前面所有的这个呃

所有的这个 P 幸运值到 pk 要相互垂直

要相互共轭

那么怎么去构造呢

就是其实啊写出来非常简单

就是我去算一个贝塔 K

然后再 pk 加一等于 2KRK 加一

加上贝塔 K 乘以 pk

所以它也是一个一步更新的东西

那么这个东西就涉及到这个 QUOB 子空间的

一些呃独特的性质

这个就是我们今天就不展开讲了

所以说这个就是共轭梯度算法的一个

基本的一个流程

总体来说共有梯度算法呢

还是一个写起来非常简单的一个东西

并且呢它的效率也是非常高的

OK 那么关于呃过期的算法呢

它的这个具体的数学大家没有了解

就没有没有太听懂也没有关系

就是呃因为很多时候大家只需要用就可以了

对吧

那么啊呃这个具体的这个就比如说

但是大概的这个思想呢

就是我们刚才说的这个子空间迭代的这个思想

就是这部分应该是额

就可以稍微稍微稍微了解一下

然后具至于说具体这个阿尔法 K

贝塔 K 是怎么算的的话

这个就是再细节的数学推导

这个就大家有兴趣的话可以去了解一下

没有兴趣的话

其实就好只要会用就可以了

### 01:08:04,50

![01-08-04](tmp/01-08-04.jpg)

那么共和梯度算法呢

它是一个非常快的收敛算法

它能够保证在 N 部的时候

N 部内就能收敛到最终解

而且呢一般来说我们不需要真的让它迭代 N 部

我们只需要迭代小呃

就是远小于不是小于 N 部的

这样一个一些一些步数的时候呢

它的这个误差

基本上就能收敛到这个阈值以下了

那么对于常见的这种几万维的矩阵来说的话

我们其实只需要几千步就可以啊

让共和梯度法收敛的差不多了

那并且呢共和梯做法

他有一个额收敛速度的上界

写成下面这个形式

那么这个形式呢 SK 减一减去 X 星

X 星是我们的真实解

SK 是迭代到第 K 步的解

XK 减一减去 X 新的模长

也就是 DK5 的误差

这 K 步的误差比上初始的误差

这个比值呢它是会小于后面这样一个啊

一个一个幂指数

KK 是在这个指数上

然后里面的这个卡帕减一除以卡帕加一呢

它是一个常数

其中这个卡帕是矩阵的条件数

它定义为 A 矩阵的最大特征值与最小特征值的

模长的比值

那么那么一定有这个卡帕是大于等于一的

那么这样一个比值

如果 A 的这个条件数比较大的时候呢

比如很大很大 1000 多的话

那么这个卡巴减一除以卡帕加一的时候

他就会趋向于一

如果卡帕趋向于呃

卡帕趋向于一的时候

就是条件数趋向于一的时候

那么哪怕减一

它就是一个很小的值

那么这个值就会趋向于零

换句话说什么意思呢

就是说如果 A 矩阵它的条件数越小

那么这个额幂指数的这个底数它就越小

越接近于零

那么整个收敛就会越快

像我每次都乘以

比如说 0.10.001

0.0001 这样来收敛

但是如果呢这个条件数它非常的很大的话

那么后面这个值就会接近于一

现在我每次都只乘了

比如说 0.0.99

每次只剩 0.99

那这时候他的收敛就会很慢了

所以说混合梯度法

他的这个收敛速度主要相关的条件

设相关的就是这个矩阵的条件数

一般来说

我们希望这个矩阵的条件数要尽可能接近一

这样能保证公共梯度算法

他的这个收敛是非常快的

### 01:10:50,50

![01-10-50](tmp/01-10-50.jpg)

那么为了让公有梯度算法

他的这个条件数变小

有一种方法叫这个预处理

叫 pre conditioning

我们怎么去改善它的条件数呢

我们可以尽可能去找到这个

A 逆的这样一个矩阵

它的一个近似矩阵 M 等于 P 转置乘以 P

那么我们去转而求解下面这样一个矩阵方程

那么这样一个矩阵方程

它跟 ax 等于 B 显然是等价的对吧

并且呢这个 PAGAPT 这样一个矩阵

它的条件数一定是比 A 更啊

一定是比

一定是比原来那个 A 矩阵单独要更小的

为什么呢

因为在极端情况下

如果我们考虑这个 M 矩阵

它就是 A 逆的话

那么上面这个矩阵的条件数

那就应该是一对吧

所以说如果我们能找到 A 逆的这样一个

近似矩阵 M

那么我们转而去求解上面这个矩阵的话

矩阵方程的话

那么它对于 cg 来说就会收敛的非常快了

那么如何去选择 M 呢

其实跟我们刚才介绍这个不动点迭代

有一些类似对吧

不动迭代我们说到 JACCOON 高三了对吧

那么这个时候这个 M 矩阵我们也可以选择

比如像对角矩阵

我们直接把 M 选成 A 的对角

那这个时候对应的这个额 cg 方法

就成为这个白 ano precondition 的 cg DP c g

同时呢我们也可以用啊

呃用最开始介绍这个 LLT 分解

也是 CHOLEICITY 分解的一个简化的形式

叫不完全的 CHAI 分解

那么对应的这个 CDCD 算法

就称为这个不完全的查尔斯 D 分解的 cg

也就是 incomplete choleski preconprecondition

cg 就是 i c p cg

那么从右边这个图里面可以看出来

这个 ICPCG 呢

它的收敛速度跟没有非常性能的时候呢

他这个收敛速度是快非常多的对吧

所以 PRECONTAIN

preconditioning 是一个非常强大的一个技术

那么 ICPCG 呢

也是一个

我们在求解这个正定的对称的矩阵方程的时候

非常常用的一个求解器

一旦你的方程是一个对称的半正定的矩阵的话

你可以试一试 ICPCG

那它的结果一般收敛速度是很快的

对于一个这个啊几万维的这样一个矩阵

你甚至可以只需要这个几十步

就可以让它收敛到不错

### 01:13:38,50

![01-13-38](tmp/01-13-38.jpg)

OK 那我们呃介绍这个双就可以 grain 的嗯

这个啊 cg 迭代之后呢

那么这个 CD 迭代

它只适用于这个对称正定的这个矩阵方程

那么其实 clot 子空间方法本身

它是有呃其他形式的

来适用于更啊更宽泛的情况

那么这地方列出了一些常用的

比如说如果我们求解的是一个对称矩阵

但是不一定是正定的矩阵的话

我们可以考虑使用这个 MRS 求解器

一个是 minimal residual 方法

那如果我再求解的是一更一般的矩阵

它甚至都不是对称的话

也有对应的 clot 子空间求解器

比如说这个啊 GM2S 以及 by cg 以及 by cg step

那么这些求解器它都能求解这个呃

一般的非对称的矩阵方程

那只不过呢如果我们用比如说用这个啊 JIMRSS

或者说明 RSS

去求解这个对称正定的这个矩阵方程的时候

它的速度呢不会就会比这个 cg 要慢一些

所以为了速度的话

你就去看一看这个矩阵到底是一个什么样的

一个矩阵

如果是对称正定的话

那么 CD 就是最优的

如果只是对称的话

那 AMMRS 是最有的

如果是一般情况下

再考虑这个 gm rs 和这个 by cg step 之类的

一些求解器

### 01:15:11,50

![01-15-11](tmp/01-15-11.jpg)

Ok

那我们可以总结一下今天介绍的这啊

其实三类的求解器对吧

那我们的结论是什么呢

是说如果我们的矩阵规模比较小

比如说小于 1000 为

那么通常直接求解是一个最快的方法

就比如说像这个有 RLU 分解

比如说 LOT 分解

对 OT 分解只剩矩阵是这个对称正定的情况

那当矩阵规模比较大的时候呢

这个直接求解

它的速度就会由于它的这个复杂度的限制

就会非常的慢

那这个时候迭代球鞋气息会变得更有优势一些

那迭代求解器又分为两种

第一种我们是这个称为这个不动点迭代

比如说像这个折扣 be iteration 和高 sao iteration

它的优点就在于它的实现是非常简单的

但它的收敛速度其实是不一定很快的

一般来说我们需要去调节这个松弛系数

去平衡这个啊绷点迭代的稳定性和收敛速度

然后不动点迭代呢

它同时可以可以被改进为这个多重网格方法

我们今天没有介绍

但是呃呃我们需要了解

就是说这个多重网格方法

其实它的速度是非常快的

呃然后呢

另外一类迭代方法

就是这个 CROLOVES 子空间的方法

它一般收敛的速度呢是很快的

然后呢

以及我们需要对不同的这个矩阵去使用

相对应的方法

比如说如果是对称正定的矩阵

我们用 cg 对称矩阵

我们用明 rise

然后一般矩阵用这个 gm rise 和 by cg 之类的

算之类的算法

然并且呢如果我们能够对矩阵进行预处理的话

那么这个时候能大大提高这个呃 FLO 子空间

迭代方法的这些的它的收敛速度

比如说我们之前介绍的这个 DP

cg 和 sap cg 方法等等

所以大家可以看到来说

就是我们解决这样一个 X 等于 B 的呃

问题来说的话

他其实没有一个通用的一个最优的一个

一个 SOFER

更多时候还是得该大家

根据自己的这个实际情况

去选择合适的这个啊求解器

那么所有今天提到这些求解器

在这个呃像啊 N 派呀

S i p y

还有这个其他的一些啊 MATLAB 呀

这些数据库里面都是有内部的这个实现

大家就可以啊去掉那些函数的接口

然后它的名字呢

一般来说也就是啊非常的这个直观

比如 LU 一般来说就直接就是 LU

然后 LT 呢可能叫这个 CHOLESKI

然后 MRS 或者什么 cg 呢

一般就直接叫叫 cg 或者 MRS

所以说啊就是大家在以后用这些 soft 的时候

可以结合自己这个矩阵的这个特性

然后去选择这个最合理的

最快的这个扫分

### 01:18:06,50

![01-18-06](tmp/01-18-06.jpg)

OK 那我们今天就介绍到这里

然后作业的话就是呃

给大家就是几个代码填空题

就没有理论作业了

然后那个大家可以去啊

就是那个代码填空其实非常简单

就大家就对应这个 PPT 里的这个算法流程

把那个对应的空给补上就可以

然后大家可以跑一跑看一看

玩一玩

来比较一下不同的这个热水器的

它们之间的这个速度差别

OK 那我们今天就介绍到这里

### 01:18:33,0

![01-18-33](tmp/01-18-33.jpg)

答案有什么问题吗

## Questions

- 什么是对称半正定矩阵？
- 什么是特征值和复数特征值？
- 什么是谱半径？
- 什么正定矩阵？

## Resources

关于线性系统相关数学推导：[Housz 的杂货铺](https://www.zhihu.com/column/c_1263955087426830336)
共轭梯度法：[共轭梯度法通俗讲义](https://flat2010.github.io/2018/10/26/%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95%E9%80%9A%E4%BF%97%E8%AE%B2%E4%B9%89/#1-%E7%AE%80%E4%BB%8B)
