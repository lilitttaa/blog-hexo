---
title: ML for Game Developer
category:
 - AI
sortValue: 110000
---

- [Machine Learning for Game Devs](https://www.ea.com/seed/news/machine-learning-game-devs-part-1)


### 00:00:04,80

![00-00-04](tmp/00-00-04.jpg)

大家好。今天，我将为游戏开发人员介绍机器学习。

我这样做的动机是，当我们游戏开发人员工作时，那里的机器学习信息也是为使用 python 工作的人量身定制的，他们只是完全不同的世界。

但是在这里，我们有很多机器学习专家，他们从事很多游戏开发。

实际上，可以稍微弥合这个差距，并制作机器学习的黑匣子。

少一点。黑匣子。


### 00:00:39,12

![00-00-39](tmp/00-00-39.jpg)

首先，这是概述。我们首先要讨论多层感知器，这是经典的神经网络类型。

我们又来了。然后我们将讨论梯度下降，这是您训练神经网络的方式，然后我们将讨论计算梯度的 3 种方法。

在那之后，我们将把它们放在一起。我将展示如何训练神经网络识别手写数字的秘诀。

然后如何在实践中实际使用它，培训、应用程序只是 dental 和 c plus plus，没有外部库。

而实际使用它的 demo 又是，你知道的，在 Dx 中。

12，只是非常简单，很容易看到发生了什么。

好的，首先，我们来谈谈多层感知器。


### 00:01:37,68

![00-01-37](tmp/00-01-37.jpg)

又名像原版神经网络或经典神经网络。

所以，如果你看右边，我们那里有一个神经网络，它有 2 层，一层，只是一组神经元。

左侧是输入层，右侧是输出层，每一层都必须连接两层中的所有神经元。

这意味着它是完全连接的。如果你看到 W，那就是权重，而权重只是节点之间连接的强度。

所以，就像你有一个输入，你试图计算一个输出，你把输入乘以输入。

哦，其中一个神经元的权重，你把这些都加起来。

然后你有偏差，即 B。

该神经元的偏差为您提供最终值。

所以它就像一个标量值。

Neuron 资产输出。

你通过一个激活函数来放置它，它就像一个 just， some， any nonlinear thing like take the value 的符号。

然后继续下一件事。所以在感知器中，它们采用一个平坦因子，其中是输入神经元值。

它给你一个整数 out。所以他们的二进制分类器，我的意思是，输出要么是 0 和 1，要么你要做这样的事情，比如说，最大的输出是赢家，这将是我们给我们输出的整数的索引。

当我们训练这些网络时，我们会给它们一个输入和一个该输入的预期输出，这就是监督学习。

这与无监督学习形成鲜明对比，后者就像你一样。

给它一堆数据，你试着告诉它尝试对它进行分类，然后像 3 个不同的组一样。

而且你不在乎那些无人监督的小组是如何组织的。

学习。当您遍历整个训练数据集时，它曾经称为 epoch。

在培训期间。在训练过程中通常会有几个 epoch，如果你在任何 epoc 期间多次更新权重和偏差，这称为迷你批处理，你在训练期间更新权重的程度称为学习率，我们将在继续讨论其中的一些细节。但我只是想给你一些基本的术语。


### 00:04:17,68

![00-04-17](tmp/00-04-17.jpg)

好的，关于你如何实际评估或执行神经网络（也称为推理）的更多信息是，如果你试图计算特定输出神经元的输出，你会找到连接到该神经元的所有东西，然后他将权重乘以进入该神经元的神经元的值。

所以，你知道，对于输出 1。它的输入将是该输出中相应权重的 1 倍、2 倍和 3 倍。

之后，您将为输出添加偏置项。

一个神经元，然后你将使用非线性激活函数来获得最终值，对于感知器来说，激活函数基本上只是一个，如果值大于 0，则结果为 1，我将它设置为 0，从现在开始，我将将其称为符号函数，即使如果考虑符号函数，它可能不是你所想到的。

所以，是的，这就是你计算神经元输出的方式，也就是说，它并不太复杂。


### 00:05:26,72

![00-05-26](tmp/00-05-26.jpg)

但就这么简单，如果我们把 input 向量加到 1 到末尾，我们实际上可以把它简化得更简单一点。

然后我们获取每个神经元的权重向量并为其添加偏差。

我们实际上可以通过做一个点积，然后对其执行激活函数来获得输出。

你可以看出，在左侧的第一次代码检查中，如果你想在这里获取两个输出值，你可以做 2 个点积，这只是一个矩阵乘法。

所以在第二次代码检查中，在左侧，在底部，你可以看到我们在哪里做一个多矩阵乘法来得到输出，然后我们将激活函数应用于输出的每个部分。

因此，如您所见，评估神经网络中的神经元或神经网络中的层非常简单。

还因为它是矩阵乘法。它也对 Gpu 非常友好。


### 00:06:28,16

![00-06-28](tmp/00-06-28.jpg)

好。所以现在是第一个练习的时间，这个演示文稿中将有 4 个练习。

基本上，这个想法是你可以传递视频。去吧，看看解决方案，然后再回来。

所以在这个练习中，我们以右侧的感知器为例，它有 2 种类型。

顶部有 2 个输入和一个输出，底部有一个输入和一个输出。

你要拿着这些，用它们来做逻辑。

但是传统的 or 和 not 的教育，然后也是独家的 or 和 我在幻灯片上有一个链接，上面说哪里有更多的说明，解决方案是继续。

暂停视频并尝试一下，然后回来。

好。凉。欢迎。返回。如果你试一试。如果没有，我们将介绍本练习的答案。

所以，我们开始吧。好。所以有多种方法可以解决这些问题。


### 00:07:25,76

![00-07-25](tmp/00-07-25.jpg)

所以，如果你的看起来不同，但仍然有效。这完全没问题。

所以，或者说我所做的是，我将权重都设置为 1，将偏差设置为 0，最终结果只是我们将输入 1 添加到输入 2 中。

这些东西可以是 0 或 1。然后我们让它们通过 sign 函数。

因此，如果您查看右侧的真值表，您会发现它为我们提供了所需的结果。

嘿！晴朗？权重又是在我的解决方案中。

但这次我将偏差从 0 改为负值。

所以现在它把输入和输出相加，然后减一。

所以这条路。它要求两个 inputs 都为 1 才能大于 0。

因此，一个是输出。


### 00:08:23,84

![00-08-23](tmp/00-08-23.jpg)

嘿！接下来不是，为此我对权重做了一个负数和一个偏差 1。

所以你可以看到，如果你在 input 中插入 0，你会在 output 中得到一个 1。

如果你将那个插入 input，你会在输出中得到一个 1，你会得到一个 0 is output。

接下来，我们将检查 X 4。

如果我必须猜测，我敢打赌，如果你尝试一下，你会遇到一些问题，那是因为这是一个棘手的问题。


### 00:08:46,72

![00-08-46](tmp/00-08-46.jpg)

希望没有花太长时间试图弄清楚并把它拉出来。

你的头发，但这里有一个有趣的教训，那就是感知器实际上不能做独占或不添加隐藏层。

这是它能做的事情的局限性。

所以，通过添加隐藏层，你可以做独占 OR，这是我使用的废物，这可能看起来像是无稽之谈。

但这里有一些直觉，那就是神经元只给出的隐藏 1 是输出。

如果只有 input 1 打开，input 2 关闭然后隐藏。

2 与隐藏的 2 相反，如果输入、1 关闭且输入 2 打开，则只会给出 1 是输出，然后我们所做的是 OR隐藏的。

1 个和隐藏的 2 个放在一起，给我们最终的结果。

所以，看看真值表，你可以看到它给了我们想要的东西。

希望很酷。这很好。因此，这是一种不同的思维方式。

好。那么接下来，我们来谈谈梯度下降。


### 00:09:56,24

![00-09-56](tmp/00-09-56.jpg)

所以梯度下降是我们通常训练神经网络的方式，所以让我们来谈谈真正快速的训练。

因此，当我们评估一个神经网络时，我们给它一些输入，然后我们得到输出。

哦，当你在做训练时，你要做的是通过成本函数将输出放进去，成本函数会给你一个成本或分数，说明网络的表现如何，或者更确切地说，它给你的值是该输入的输出有多错误，所以我们的目标是调整神经网络内的权重和偏差，以给出我们给它的输入的最低成本函数。

所以，是的，这就是目标。有 2 种方法。

我要提到关于如何到达那里，所以有两种训练方式。

你可以随机生成一大堆神经网络，比如随机化权重和偏差，你可能会生成一百个、一千个、一万个或更多这样的神经网络，然后保留最擅长的那个，这对小型网络来说效果很好，但是当你有更大的网络时，它有， 你知道，成千上万的权重和偏差或更多，这变得不那么有效。

因此，人们通常会做的是，他们从一个随机网络开始，然后他们会尝试改进它。

这就是我们将要讨论的内容。


### 00:11:27,68

![00-11-27](tmp/00-11-27.jpg)

因此，为了改善网络。您正在优化成本函数。

这意味着您实际上是在尝试找到函数的最低点。

所以，如果我们看一下左侧有一条简单抛物线的地方，它就像 y 等于 X 的 F。

你可以把 X 看作是这样的。神经元的权重或偏差。

我们试图找到为我们提供曲线上最低点的值。

哦！因此，对于简单的函数，一个简单的方法是，你只需找到函数的导数，将其设置为等于 0。

然后，无论这些点在哪里，您都可以查看它们，看看它们是否是您要查找的最小值，就像迭代它们一样。

但是，如果我们看一下中间的图片，这更像是神经网络实际处理的内容，因为您尝试优化的函数的每个参数都是权重和偏差之一。

我说，经常有几千、十、几千！

更多。所以这意味着神经网络的维度通常非常高，以数千计，因为它是参数的数量。

那么，您真正能做什么呢？嗯，成本函数最终返回一个值。

这就是分数。所以就像使用现实世界的直觉一样，你可以随机地在上面放一个球。

形状，让它滚下坡，无论你最终到达哪里，它都将是最小值。

它可能不是全局最小值。这可能是当地的最低工资，但你会找到最低工资。

就像你在右边看到的那样。如果您从错误的位置开始，您最终可能会得到一个局部最小值，而不是您正在寻找的全局最小值。

但是这还是有希望的，那就是随着维度数量的增加，这个局部最小值的问题实际上会减少，因为当你陷入一个浅谷时，一个维度很可能不是所有其他维度的谷值。所以你可以滚动到一边并逃离局部最小值以尝试找到 gl，还有一种称为 Adam 的技术，它可以升级梯度下降。

它增加了小滚动的动量概念，以及自适应步长，这可以帮助您达到全局最小值。

并帮助您更快地实现它！


### 00:14:07,20

![00-14-07](tmp/00-14-07.jpg)

好吧，所以滚下坡的物理直觉很有意义。

但是你怎么做呢？数学？答案是梯度，而梯度只是函数的导数向量。

因此，如果你的函数有 2 个参数，你的 gradient，我们将有一个 will 是一个大小的因子。

2，其中每个条目都是每个参数的导数。

坡度的好处是它指向最陡峭的上坡方向，因此由于我们试图找到最小值，我们只走相反的方向，这将是最陡的下坡方向。

不幸的是，达到全球最低限度的途径往往不会是一条直线。

所以你想遵循渐变，但它不是一条直线。

您想采取小步骤并重新评估它。

所以这有点像学习率。

所有关于你沿着渐变前进的台阶的大小。

太酷了。是的，既然您知道导数如何帮助您获得函数的最小值，那么解释这就是可微编程很重要的原因可能很重要，因为如果您构成一个程序或像一个程序的一部分是可微的，这意味着

你可以优化它，使用 Gradient Descent 来找到你的参数的最佳值，无论你的目标是什么，如果你听说过可微渲染，这也是同样的想法，你可能有一个环境渲染算法，它有很多参数来说明它是如何工作的，它在哪里射击比赛等等， 你可能有一个 Ground Truth，你知道，每个像素 100 次加注，类似这样。

如果您有可微分渲染器，则可以将参数调整为可微分渲染器。

您知道，找到渲染的最低误差与真实值的全局最小值。这就是为什么可区分的租房者很重要的原因。


### 00:16:16,72

![00-16-16](tmp/00-16-16.jpg)

好的，那么继续，让我们接下来讨论 3 种不同的梯度计算方法。


### 00:16:22,88

![00-16-22](tmp/00-16-22.jpg)

所以我们从有限差分开始，这就像获取导数、成分的简单模式方法。

那么，如果你考虑一下导数是什么，它只是告诉你一个函数是什么样子的。

Y 等于 x 的 f。它告诉您当您在 x 轴上迈出一步时 y 发生了多少变化。

因此，如果我们想知道该值是什么样的，我们可以这样做。

那么，您是有限差分如何运作的呢？您知道 X 值的 Y 值。

所以你踩了 X 一步。我们评估函数，然后，无论你在 y 中改变了多少，除以你的步长，大小告诉你你的导数，或者告诉你该点线的斜率。

还有这里。Epsilon 就像你正在服用的步长，我们在顶部公式中有它的公式。

你可以想象，如果你有一个摆动函数，如果你取 2 个大的步长，它会给你错误的答案，对吧？

因此，epsilon 值越小，您将获得的导数就越准确。

当他接近 0 时，我们处于极限，您将得到精确的导数。

但是，由于我们使用的是计算机和浮点，因此您也不想做得太小，因为那样您就会遇到浮点问题。

因此，它可能有点像一个微妙的行为，试图为您的用例找到合适的 epsilon。

这就是前向差异。但实际上还有另一种叫做中心差异的东西，也就是说，你不仅仅是在 X 上向前迈出一步，而是向后退一步。

所以现在你有 2 个不同的 Y 值围绕你在 X 轴上的位置，你除以两倍的步距。

这更昂贵，但也更准确，如果你只看公式，看起来我们只是在计算 F 两次。

在这两种情况下，为什么它会更贵呢？但原因是很多时候，尤其是在神经网络中，你已经有了 F of X 的值，所以当你必须做 F of X 加一个步骤时，这只是一个额外的计算，其中有中心差异，是的。

您将不得不执行 2.


### 00:18:53,36

![00-18-53](tmp/00-18-53.jpg)

好。第二次练习的凉爽时间。这又在 Gitlab 仓库中。

有说明和解决方案文档可供尝试。

所以去吧。暂停视频，试一试。

好的，欢迎回来。我们不打算回顾其余的练习，因为它们有点像探索。

为您提供一些实践经验。是的，就像你选择的探索方式会与其他人不同。

好的，从财务开始，我们可以做数字。


### 00:19:32,88

![00-19-32](tmp/00-19-32.jpg)

这是计算梯度的第二种方法。而这也被称为 mode 自动微分，所以首先，我们要讨论对偶数，然后我们要讨论如何使用它们来获得梯度。

所以对偶数与复数或虚数非常相似。

那些家伙没有。I 平方等于？消极的。

但是对于对偶数，我们有一个 epsilon 平方等于 0，重要的是，epsilon 本身不是 0。

所以一种看待它的方法是，Epsilon 只是大于 0 的最小数字，因此当你平方时它确实变成了 0。

这听起来可能很像我们谈论 epsilon 的方式。

足够了，当 epsilon 接近 0 时，你会得到正确的值。

这里的想法是一样的，但 Epsilon 也可以用 matrix 表示，正如你在幻灯片上看到的那样。

如果将该矩阵与自身相乘，则得到 0 矩阵。

太像复数了。工具编号的形式是 a 加 b 乘以 epsilon，其中 a 是实部，B 是对偶部分，因此使用工具编号得到导数的形式是 a 加上 epsilon，您将 a 设置为 X 值，将 B 设置为 X 的导数， 这只是一个。

所以你知道，你的值加上 epsilon，然后，当你把它代入你的方程式中时，你可以像往常一样做数学，你唯一需要注意的是，如果你有 E 的平方或任何其他更高功率的幂，V 你把你说的项设置为等于 0，当你做完所有你说的你就知道你的简单代数了。

结果将是 a 加 b epsilon，a 将是 X 处的函数值，B 将是导数。

很酷的是，这不是一个数值方法。

这为您提供了精确的导数，因此它比无限差值更好。

这样，你可以扩展它，一次给你多个变量的导数，以及高阶导数，比如，如果你想要二阶导数或类似的东西。


### 00:21:51,92

![00-21-51](tmp/00-21-51.jpg)

因此，让我们快速通过一个示例来了解它是如何工作的。

所以我们有。让我们以函数 3 x 加 2 为例，它只是一条线。

假设我们想在 F. 4 处计算它。

我们还希望获得 F 为 4 的导数。

所以我们首先要做的是，我们必须将 x 3 和 2 作为我们的变量和常数。

所以对 X 来说也是如此。值为 4，导数为 1，因此 3 和 2 的 3 等于 4 加 1 个 epsilon。

它们是常数。所以我们将 real 部分设置为它们的值，并将 dual 数字设置为 0。

对偶部分为 0，因为常数的导数为 0。

所以，如果我们将其代入方程式中，只做简单的代数，确保任何 E 平方项都是 0，我们得到的结果是 14 加 3 个 epsilon。

现在很酷的是 14，这是真正的部分，是我们函数的值。

F.的 4，而 3 是 4 的导数 F。

所以，是的，它奏效了。如果您想自己尝试一下。

这是一件有趣的小事，就像让它方程式尝试插入双数，看看你是否得到正确的答案。

向上。但是，对偶数就像常规数一样工作，一个很酷的一点是，你可以创建一个类，就像复数有一个类或一个结构体一样，然后对所有数学内容进行重叠运算符重载，然后这将允许你做的是，如果你编写了一个函数来接受一些参数并返回一个浮点值。

您可以将该函数从使用浮点数更改为使用模板化类型，然后只需传入工具编号 X 值，或者任何参数，它就会照常运行函数。

但是当你完成时，你将同时拥有你想要的值和导数，这非常酷。

这使得使用双数成为一种很好的方式。

好。所以是时候进行第三个练习了，这个练习，同样在 repo 中。

有一些说明，解决方案是像上次一样的代码练习。

在这个应用程序中，您将实现加法、减法和乘法以及对偶数，以便它能够进行梯度下降并优化函数。

所以去吧，试一试，然后再回来。

欢迎回来，这主要是为了探索。

那里有类似的解决方案，因此我们不会在这里详细介绍。


### 00:24:43,36

![00-24-43](tmp/00-24-43.jpg)

好。最后但并非最不重要的一点是，计算梯度的第三种方法是通过反向传播。

这有点像 promote 渐变。这是人们实际使用的大部分内容，归根结底是使用链式规则。

如果你已经有一段时间没有学过微积分了，那么链式法则只是说你可以对导数进行交叉抵消的花哨方式，就像你可以做一样，如果它们是分数。

所以，是的，让我们看看怎么做。

因此，如果我们考虑右侧神经网络的成本函数，它就像一堆。

它有点像几个嵌套的子函数。

所以为了计算成本，我们通过成本函数来激活输出神经元，而得到输出函数的激活是针对输出神经元的，因为我们通过激活函数输入神经元的原始输出，然后计算神经元的原始输出是我们将输入乘以它们的权重， 把这些加在一起，加上偏见。

所以，就像。这就是你评估的方式。

但是这个神经网络。所以接下来，我们可以得到每个的导数。

对于这些嵌套步骤中的每一个，首先我们可以说，如果我们改变实际值，我们可以捕获它对成本的影响程度。

这就是我们的第一个导数，即 delta 成本除以 delta 激活，其导数只是激活值处的成本函数导数，对吧？

因此，我们可以做同样的事情来找出当原始输出发生变化时激活发生了多少变化，然后我们也可以对权重和偏差做同样的事情，就像权重 1 一样，我们得到了神经元的原始输出如何随着权重 1 的变化而变化的导数。 然后，一旦我们有了这些导数，我们实际上就可以将它们从左到右相乘。

而且，正如您在紫色中对那些导数进行一些交叉抵消时所看到的，我们只剩下 delta 权重的 Delta 成本，你知道，在英语中，意思是如果我们一个接一个地改变权重。

哦，如果我们有 1.0 等待，那会增加多少成本，这就是这个衍生品的干扰告诉你的，在紫色的右侧，你可以看到它的计算非常简单。

所以，是的，反向传播。它非常快速，非常简单。

如果已经有一段时间了，它可以进入。

yes 使用了链式规则。需要一些时间才能重新投入其中。


### 00:27:37,68

![00-27-37](tmp/00-27-37.jpg)

不过，使用反向传播时需要注意的一点是，可能有多种路径会影响 cost 函数。

就像这里一样，如果我们想得到 delta bias 的 Delta 成本 1。

所以更清楚地说，成本受我们改变的影响有多大。

那是什么时候？我们实际上有 2 条路径？因为如果我们增加偏差，那会影响隐藏在 a 中的神经元和隐藏在 B 中的神经元，因此要找出我们发现的成本是多少，我们必须将这两个想法加在一起，就像一个小细节一样。当您尝试编写自己的神经网络评估或训练时，很容易没有意识到这一点。


### 00:28:24,32

![00-28-24](tmp/00-28-24.jpg)

好吧。最后一次练习的时间。就像前几次练习一样。

这里的目标是为其提供导数，以便能够在函数上进行梯度下降。

不过，这一次，函数是嵌套的。所以你必须使用链式规则。所以来吧，试一试，只要你准备好了就回来。

欢迎再次回来，那么让我们比较一下我们讨论的 3 种快速计算梯度的方法。


### 00:28:51,44

![00-28-51](tmp/00-28-51.jpg)

所以，有限差分非常酷。您实际上不需要了解函数的内部结构。

您会收到很多类似的东西，可能是对 Dll 的调用，甚至是通过网络进行的调用，您甚至无法查看程序集。

它可以是一个完全的黑匣子，你可以得到参数的导数，这很酷。

但是，一个问题是它只是一个近似值。

由于您必须对所需的每个变量进行一两次评估，因此它的导数可能会变得非常慢，尤其是对于至少具有数千个参数的神经网络。

对偶数要好一些，它们会给你精确的导数。

不幸的是，你正在处理的函数在实践中通常不是什么大问题，因为你需要能够修改该函数以使其采用模板类型，以便你可以给它两个数字，但这样做的缺点是工具编号也非常慢， 而且它们会占用大量内存，因为，假设您正在评估一个具有 10,000 个参数的网络，例如权重和偏差。

这意味着每一个单一的、非常真实的、临时的、局部的，所有这些事情都会让你知道，里面有很多漂浮物，你知道，数以万计的漂浮物。

所以这是大量的内存。当你做加法、减法、乘法时，它必须经过所有这些浮点数。

并对它们进行操作。所以从这个角度来看不是很好，最后是反向传播，它非常快，我们很快就会看到。

但它也为你提供了精确的导数，就像对偶数一样，反向传播的缺点是你真的需要深度积分和对你试图获得梯度的函数有深入的了解，这样你就可以在每个步骤中执行链式规则，但要选择你用来计算梯度的方法。

这不像是全有或全无的事情。您可以选择执行像 Overall 这样的反向传播。

但是，如果它是什么，你正在优化，很难或不可能从你可以使用其他方法之一（例如对偶数或有限差分）获得要包含在反向传播中的导数。是的，即使是一些像这样的，你知道的，一些大型 python 包实际上会这样做。他们会把这些混在一起，因为它们在情境上是最好的。


### 00:31:51,92

![00-31-51](tmp/00-31-51.jpg)

好了，就这样了。就像先决条件信息一样，要真正能够为实际用例进行一些神经网络的训练和执行，那么让我们开始吧。


### 00:32:05,76

![00-32-05](tmp/00-32-05.jpg)

因此，首先讨论如何设置网络并对其进行训练，我们将在称为 Mnist 数据集的东西上训练网络，这是一堆手绘数字。

我在这张幻灯片上包含了一些，显示了一堆零。

这些图像都是 28 x 28 的，所以这意味着，如果我们想将这些图像插入到神经网络中，那就是 784 像素。

所以我们需要 784 个输入神经元。

除了输入层之外，我们还将有一个包含 30 个神经元的隐藏层。

然后，当我们评估网络时，我们将有一个包含 10 个神经元的输出层。

具有最大输出值的 neuron 输出将被视为获胜者，并且该索引将是网络已识别的值。

所以，你知道，如果 outlook 0 具有最大的值，那么我们会说网络将其识别为 0。

如果输出 5.最大的价值是什么？

然后我们假设网络引用 5，所以把所有层加起来，就是 824 个神经元。

正是将所有连接和偏差相加，才产生了近 24,000 个参数，我们正在努力优化，这是相当多的。

但就神经网络而言，这实际上是一个相当小的网络。

如果你能相信的话。好。这就是拓扑结构，有点像整体，我们要为训练 Ms 数据集的网络做 2 个数据子集。

有 60,000 位数字用于训练网络。

然后有 10,000 个用于在训练后测试网络，所以我们在这里做的是，我们使用 60,000 个进行训练，然后我们评估结果的质量。

使用这 10,000 个。我们这样做的原因是神经网络是另一种优化技术。

它们有时会过度拟合训练数据。这意味着他们非常擅长训练数据。

但他们不擅长其他任何事情。因此，如果你拿着你的训练数据，并保留一些东西来测试你正在做的事情，你就允许自己在以前从未见过的数据上测试网络，这将告诉你它是否结束，是否适合。

因此，对于我们的激活函数，我们将使用病态点，它本质上是一个可微分的阶跃函数，而不是我们目前一直在讨论的符号函数。

如果你知道脚趾映射，它可能看起来非常相似，这就是我们所做的，我们基本上是把所有可能的浮点数都映射到，你知道的，对于步骤类型的情况，它们是 0 到 1 的映射。

当我们进行训练时，我们将执行 30 个 epoch。

因此，我们将对这 60,000 个号码进行 30 次检查。

我们将对数字的顺序进行随机排序。

每个 epoch 中，我们将有一个 10 的小批量大小，这意味着当我们每 10 个项目运行这 60,000 个训练项时，我们将获得成本函数的平均梯度，然后我们将调整我们的权重和偏差。我们将使用 3 点 O 的训练学习率，这意味着我们将梯度乘以 3，然后再从权重和偏差中减去它。

然后，对于我们整个网络的成本函数，我们只需将 10 个输出神经元中的每一个的成本函数相加，单个输出神经元的成本函数将是所需输出的一半。

减去实际的输出平方，或者换句话说，它是误差平方的一半。

我们添加了 one half，因为当你取导数时，它会用平方抵消 2 的幂。所以这意味着你的导数实际上只是实际输出减去期望值，这很好，也很简单。


### 00:36:26,24

![00-36-26](tmp/00-36-26.jpg)

好的，我来给你展示这个东西的实际效果。

但首先还有几件事。我用 3 种梯度方法中的每一种来训练这个东西，我想谈谈我们所做的那些方法，这些方法最终在 10,000 张训练数据图像上获得了 95% 的准确率，从这个角度来看。

这意味着每 20 个就出局。它出错了，这是一个相当大的错误率。但这种就像玩具级的神经网络，所以结果相当不错，因为，就像一个玩具。

但是，是的，有一个更大的网络和卷积神经网络以及不同的激活函数。

这样可以为您带来比这更好的结果。因此，在有限差分的训练中，我们基本上必须评估函数。

24,000 次来获取每个输入的每个梯度，这有点多。

还有我。所以我使用 Omp 将其分散到 18 个课程中，并且还有一些其他优化。

而且还是花了 6 小时。是的。使用中心差分，这需要两倍于函数评估的数量，而一个函数评估需要 11 个半小时。

所以训练的时间相当长。这个非常小的神经网络，使用工具编号来代替来获得梯度，一旦我用天真的双数实现来尝试它，我就破坏了堆栈，因为每个变量的临时局部成本以及所有的东西，它里面有 24,000 个浮点数，所以我就像立即用完了空间。

所以我做了一个稀疏工具编号类，并编造了一个稀疏拉取的堆栈分配器，然后用 theiler 尽可能地打击它，但仍然只能把它减少到 28 个半小时，希望一天有点疯狂。是的，回来了。它只用了 90 秒。我也不需要做任何特别的事情来进行分析。

所以我认为这只是表明宏观优化可以比微观优化大得多。

你知道的，我们都知道。但是在 Gpu 上尝试这些东西并查看它们的特性是否发生了任何变化也很有趣。


### 00:38:56,72

![00-38-56](tmp/00-38-56.jpg)

所以，下一步我将展示演示。但我想快速展示一下我实际上是如何在演示中通过神经网络运行的，这是蓝色框所在的渲染图。

资源（如纹理和缓冲区）以及橙色框是计算着色器。

我已经圈出了 2 个计算着色器，它们是实际执行神经网络评估的着色器。

只是那 2 个并插入它们。我圈出的这个缓冲区是 93 kB 的长笛。

这些是由训练应用程序训练的权重和设备，只需将其加载到 GPU 的内存中，然后馈送到这些计算份额。其余的 mutute 着色器、纹理和缓冲区基本上都用于 Ui。


### 00:39:52,40

![00-39-52](tmp/00-39-52.jpg)

所以，是的，这里，这是在我们实际启动它之前的示例，我想在分析窗口中向您展示。

所以我们每秒总共使用 283 个麦克风。

因此，不到三分之一毫秒即可识别该数字。

这是在 30， 90 上。但是，是的，超级快。因此，让我实际启动演示并实时向您展示它。


### 00:40:18,64

![00-40-18](tmp/00-40-18.jpg)


### 00:40:22,40

![00-40-22](tmp/00-40-22.jpg)

好的，太好了。所以这是演示。你可以只用鼠标左键绘图，然后看到它识别 0。

一个 1，一个 2，当我清除它时，重要的是要注意它何时什么都没有得到。

如果认为它有一个 5.这很有趣，但如果你仔细想想，我们只用数字来训练网络。

我们从来没有给它任何无效的东西，也没有给它任何方法来表示某件事是无效的，所以它必须做出猜测。

所以 5 和任何数字一样好，我猜当我画一个数字时，这里显示的其他东西。

这里的第一个框显示了 784 个输入神经元。

安排他的像素。下一列框。

这是隐藏层，最后一列框是输出层。

是的，当你分析它时，你可以在这里看到隐藏层实际上花费的时间更少。

它看起来总共有 100 50 微秒，这真是超级快，超级超级快。

所以，是的，这就是演示。


### 00:41:38,16

![00-41-38](tmp/00-41-38.jpg)


### 00:41:40,96

![00-41-40](tmp/00-41-40.jpg)

您可以触发演示。它只是 c 加 dx 12。

没有外部库，所以你应该能够，只是，你知道的，打开它并编译它。

我在 Visual Studio 2022 中做到了，所以如果你使用的是 2019，我可能会抱怨。

但只要告诉它它可以转换。所以最后，希望你能看到这些东西并不那么可怕，就像反向传播可能是最相同的部分。

但这并没有那么糟糕，如果需要，您可以随时选择类似的选项。

您知道并只需支付更长的培训时间的成本。

但实际上，我想在这里展示的重要一点是。

此外，蒸汽很好。所有这些东西是，虽然训练神经网络和类似的东西可能真的很慢，但实际上在运行时使用它们可能会非常快，因为，你知道，这只是一些矩阵乘法，你知道的，那些非常快，尤其是在 Gpu 上，所以我觉得我想让你知道，你不应该害怕运行这些运行时的东西， 尽管训练需要很长时间，如果你想知道，有点像你喜欢的机器学习知识 One Way 的下一步去哪里。

接下来，你实际上可以修改 Kitl 存储库中的训练应用程序，你可以尝试为学习增加动力，或者，你知道的，去阅读 Adam 是如何工作的，并尝试实现它，你也可以尝试不同的激活函数，如 Relu

或 Sellu，特别是对于隐藏层，然后你也可以尝试使用卷积神经网络而不是我们所做的完全连接的感知器类型的网络，卷积神经网络往往在处理图像时做得更好，这正是 Ms 数据集，所以应该在那里工作得更好。你也可以尝试用不同的东西进行交易，比如，还有另一个训练集，比如 mnist，但它叫做 C far 10，它只是一些彩色图像，你可以训练一个网络。

喜欢。它们都有标签，如果你尝试并得到更好的结果或更差的结果，很高兴听到你的意见。

最后，还有一个叫做 onyx 运行时的东西，我们有一个 Ea 包装器。

这基本上是帮助你运行推理，或者更确切地说，在 Runtime 更轻松、更高效地执行你的神经网络。

所以我们确实有一个合适的说唱者。

它被用于 fea 等游戏。

但它并不是特定于冻伤的。所以如果你不使用 frostbite，你就会对此感兴趣。

这是你可以做的。你可以使用，这很酷，最后，我想说，你知道，我们 EA 有很多游戏开发专家，我们也有很多机器学习专家。

但是两者之间没有太多重叠，因为就像一个是裸机 c plus plus 和 Gpu 优化。

对于 game def，另一个就像在 Python 世界中工作。

你知道高等数学和研究论文，没有一本书有很多重叠，但我真的觉得，如果我们能从两方面帮助弥合我们两个世界之间的差距，我们真的可以做一些了不起的事情。

所以我认为，机器学习 Sig 是一个很好的地方。所以希望我们能在那里进行一些对话，并尝试建立一座桥梁，并做一些非常酷的 Next Gen. Stuff。

就这样。非常感谢你们的聆听。

打我。如果您有任何问题。


### 00:45:38,16

![00-45-38](tmp/00-45-38.jpg)


