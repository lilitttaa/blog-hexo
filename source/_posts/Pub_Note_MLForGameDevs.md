---
title: ML for Game Developer
category:
 - AI
sortValue: 110000
---

- [Machine Learning for Game Devs](https://www.ea.com/seed/news/machine-learning-game-devs-part-1)

整个 Talk 的结构：
![00-00-39](00-00-39.jpg)

- 多层感知机（经典的神经网络）
- 梯度下降
- 识别手写数字

一些基本的术语：
![00-01-37](00-01-37.jpg)

![00-04-17](00-04-17.jpg)

- 执行神经网络也称为推理
- 对于感知机，简单的使用符号函数（Sign Function）作为激活函数

![00-05-26](00-05-26.jpg)

- input 向量后面加上 1，weight 向量后面加上 bias，这样就可以把一个 neuron 的计算简化为一个点积
- 再进一步，整个 Layer 的所有 neuron 的计算可以写作一个矩阵乘法
- 因为它是矩阵乘法，所以它也对 Gpu 非常友好。

多层感知机可以表示出基本的逻辑运算：
![00-07-25](00-07-25.jpg)

- Weight 为 1, 1，Bias 为 0，这样就可以实现 OR（实际上有很多种取值方式）

![00-08-23](00-08-23.jpg)

- 表示 NOT

![00-08-46](00-08-46.jpg)

- 表示 XOR 要复杂一点，需要引入一个隐藏层

接下来谈论梯度下降（Gradient Descent）：

![00-09-56](00-09-56.jpg)

- 梯度下降是我们通常训练神经网络的方式
- 两种训练方式：
  - 随机生成一大堆神经网络，然后保留最好的那个（对于更大的网络不太适用）
  - 从一个随机网络开始，然后尝试改进它（梯度下降）

![00-11-27](00-11-27.jpg)

- 实际上是想要找到 Cost 函数的最低点
- 通常神经网络的维度非常高（取决于参数的数量）
- 就像在现实中一样，你可以随机地在上面放一个球，让它滚下坡
- 但是这个方法可能会陷入局部最小值，而不是全局最小值。不过随着维度的增加，局部最小值的问题会减少，因为一个维度的低谷很可能不是其他维度的低谷。
- 另外还有一种叫做 Adam 的技术，它可以优化梯度下降，增加了小滚动的动量概念，以及自适应步长，这可以帮助达到全局最小值。

![00-14-07](00-14-07.jpg)

- 滚下坡在数学上就是梯度，也就是函数的导数向量
- 如果你的函数有 2 个参数，那么梯度就是这两个参数分别求导组成的向量
- 它指向最陡峭的上坡方向
- 每次沿着梯度的反方向前进一个 Step，然后不断重新的进行评估，所以这个 Step 实际上就类似于学习率
- 可微就意味着你的程序可以被优化，使用梯度下降来找到最佳参数值。例如可微渲染：计算渲染过程中的导数，让渲染过程变得可微。然后通过图像和 ground truth 之间的差异，计算目标函数，得到 loss 后，对 loss 求导得到梯度，通过梯度反向传播来优化各种场景参数。

下面介绍 3 种不同的梯度计算方法：

![00-16-22](00-16-22.jpg)

- 有限差分（Finite Difference）是一种简单的数值方法。
- 通常 epsilon 越小，梯度越准确，但是太小会导致浮点数问题。
- central difference 比 forward difference 更准确（从泰勒展开的角度来看，forward difference 是一阶展开，central difference 是二阶展开），但是计算量更大，因为通常我们已经有了 f(x) 的值。

### 00:19:32,88

![00-19-32](tmp/00-19-32.jpg)

这是计算梯度的第二种方法。而这也被称为 mode 自动微分，所以首先，我们要讨论对偶数，然后我们要讨论如何使用它们来获得梯度。

所以对偶数与复数或虚数非常相似。

那些家伙没有。I 平方等于？消极的。

但是对于对偶数，我们有一个 epsilon 平方等于 0，重要的是，epsilon 本身不是 0。

所以一种看待它的方法是，Epsilon 只是大于 0 的最小数字，因此当你平方时它确实变成了 0。

这听起来可能很像我们谈论 epsilon 的方式。

足够了，当 epsilon 接近 0 时，你会得到正确的值。

这里的想法是一样的，但 Epsilon 也可以用 matrix 表示，正如你在幻灯片上看到的那样。

如果将该矩阵与自身相乘，则得到 0 矩阵。

太像复数了。工具编号的形式是 a 加 b 乘以 epsilon，其中 a 是实部，B 是对偶部分，因此使用工具编号得到导数的形式是 a 加上 epsilon，您将 a 设置为 X 值，将 B 设置为 X 的导数， 这只是一个。

所以你知道，你的值加上 epsilon，然后，当你把它代入你的方程式中时，你可以像往常一样做数学，你唯一需要注意的是，如果你有 E 的平方或任何其他更高功率的幂，V 你把你说的项设置为等于 0，当你做完所有你说的你就知道你的简单代数了。

结果将是 a 加 b epsilon，a 将是 X 处的函数值，B 将是导数。

很酷的是，这不是一个数值方法。

这为您提供了精确的导数，因此它比无限差值更好。

这样，你可以扩展它，一次给你多个变量的导数，以及高阶导数，比如，如果你想要二阶导数或类似的东西。

### 00:21:51,92

![00-21-51](tmp/00-21-51.jpg)

因此，让我们快速通过一个示例来了解它是如何工作的。

所以我们有。让我们以函数 3 x 加 2 为例，它只是一条线。

假设我们想在 F. 4 处计算它。

我们还希望获得 F 为 4 的导数。

所以我们首先要做的是，我们必须将 x 3 和 2 作为我们的变量和常数。

所以对 X 来说也是如此。值为 4，导数为 1，因此 3 和 2 的 3 等于 4 加 1 个 epsilon。

它们是常数。所以我们将 real 部分设置为它们的值，并将 dual 数字设置为 0。

对偶部分为 0，因为常数的导数为 0。

所以，如果我们将其代入方程式中，只做简单的代数，确保任何 E 平方项都是 0，我们得到的结果是 14 加 3 个 epsilon。

现在很酷的是 14，这是真正的部分，是我们函数的值。

F.的 4，而 3 是 4 的导数 F。

所以，是的，它奏效了。如果您想自己尝试一下。

这是一件有趣的小事，就像让它方程式尝试插入双数，看看你是否得到正确的答案。

向上。但是，对偶数就像常规数一样工作，一个很酷的一点是，你可以创建一个类，就像复数有一个类或一个结构体一样，然后对所有数学内容进行重叠运算符重载，然后这将允许你做的是，如果你编写了一个函数来接受一些参数并返回一个浮点值。

您可以将该函数从使用浮点数更改为使用模板化类型，然后只需传入工具编号 X 值，或者任何参数，它就会照常运行函数。

但是当你完成时，你将同时拥有你想要的值和导数，这非常酷。

这使得使用双数成为一种很好的方式。

好。所以是时候进行第三个练习了，这个练习，同样在 repo 中。

有一些说明，解决方案是像上次一样的代码练习。

在这个应用程序中，您将实现加法、减法和乘法以及对偶数，以便它能够进行梯度下降并优化函数。

所以去吧，试一试，然后再回来。

欢迎回来，这主要是为了探索。

那里有类似的解决方案，因此我们不会在这里详细介绍。

### 00:24:43,36

![00-24-43](tmp/00-24-43.jpg)

好。最后但并非最不重要的一点是，计算梯度的第三种方法是通过反向传播。

这有点像 promote 渐变。这是人们实际使用的大部分内容，归根结底是使用链式规则。

如果你已经有一段时间没有学过微积分了，那么链式法则只是说你可以对导数进行交叉抵消的花哨方式，就像你可以做一样，如果它们是分数。

所以，是的，让我们看看怎么做。

因此，如果我们考虑右侧神经网络的成本函数，它就像一堆。

它有点像几个嵌套的子函数。

所以为了计算成本，我们通过成本函数来激活输出神经元，而得到输出函数的激活是针对输出神经元的，因为我们通过激活函数输入神经元的原始输出，然后计算神经元的原始输出是我们将输入乘以它们的权重， 把这些加在一起，加上偏见。

所以，就像。这就是你评估的方式。

但是这个神经网络。所以接下来，我们可以得到每个的导数。

对于这些嵌套步骤中的每一个，首先我们可以说，如果我们改变实际值，我们可以捕获它对成本的影响程度。

这就是我们的第一个导数，即 delta 成本除以 delta 激活，其导数只是激活值处的成本函数导数，对吧？

因此，我们可以做同样的事情来找出当原始输出发生变化时激活发生了多少变化，然后我们也可以对权重和偏差做同样的事情，就像权重 1 一样，我们得到了神经元的原始输出如何随着权重 1 的变化而变化的导数。 然后，一旦我们有了这些导数，我们实际上就可以将它们从左到右相乘。

而且，正如您在紫色中对那些导数进行一些交叉抵消时所看到的，我们只剩下 delta 权重的 Delta 成本，你知道，在英语中，意思是如果我们一个接一个地改变权重。

哦，如果我们有 1.0 等待，那会增加多少成本，这就是这个衍生品的干扰告诉你的，在紫色的右侧，你可以看到它的计算非常简单。

所以，是的，反向传播。它非常快速，非常简单。

如果已经有一段时间了，它可以进入。

yes 使用了链式规则。需要一些时间才能重新投入其中。

### 00:27:37,68

![00-27-37](tmp/00-27-37.jpg)

不过，使用反向传播时需要注意的一点是，可能有多种路径会影响 cost 函数。

就像这里一样，如果我们想得到 delta bias 的 Delta 成本 1。

所以更清楚地说，成本受我们改变的影响有多大。

那是什么时候？我们实际上有 2 条路径？因为如果我们增加偏差，那会影响隐藏在 a 中的神经元和隐藏在 B 中的神经元，因此要找出我们发现的成本是多少，我们必须将这两个想法加在一起，就像一个小细节一样。当您尝试编写自己的神经网络评估或训练时，很容易没有意识到这一点。

### 00:28:24,32

![00-28-24](tmp/00-28-24.jpg)

好吧。最后一次练习的时间。就像前几次练习一样。

这里的目标是为其提供导数，以便能够在函数上进行梯度下降。

不过，这一次，函数是嵌套的。所以你必须使用链式规则。所以来吧，试一试，只要你准备好了就回来。

欢迎再次回来，那么让我们比较一下我们讨论的 3 种快速计算梯度的方法。

### 00:28:51,44

![00-28-51](tmp/00-28-51.jpg)

所以，有限差分非常酷。您实际上不需要了解函数的内部结构。

您会收到很多类似的东西，可能是对 Dll 的调用，甚至是通过网络进行的调用，您甚至无法查看程序集。

它可以是一个完全的黑匣子，你可以得到参数的导数，这很酷。

但是，一个问题是它只是一个近似值。

由于您必须对所需的每个变量进行一两次评估，因此它的导数可能会变得非常慢，尤其是对于至少具有数千个参数的神经网络。

对偶数要好一些，它们会给你精确的导数。

不幸的是，你正在处理的函数在实践中通常不是什么大问题，因为你需要能够修改该函数以使其采用模板类型，以便你可以给它两个数字，但这样做的缺点是工具编号也非常慢， 而且它们会占用大量内存，因为，假设您正在评估一个具有 10,000 个参数的网络，例如权重和偏差。

这意味着每一个单一的、非常真实的、临时的、局部的，所有这些事情都会让你知道，里面有很多漂浮物，你知道，数以万计的漂浮物。

所以这是大量的内存。当你做加法、减法、乘法时，它必须经过所有这些浮点数。

并对它们进行操作。所以从这个角度来看不是很好，最后是反向传播，它非常快，我们很快就会看到。

但它也为你提供了精确的导数，就像对偶数一样，反向传播的缺点是你真的需要深度积分和对你试图获得梯度的函数有深入的了解，这样你就可以在每个步骤中执行链式规则，但要选择你用来计算梯度的方法。

这不像是全有或全无的事情。您可以选择执行像 Overall 这样的反向传播。

但是，如果它是什么，你正在优化，很难或不可能从你可以使用其他方法之一（例如对偶数或有限差分）获得要包含在反向传播中的导数。是的，即使是一些像这样的，你知道的，一些大型 python 包实际上会这样做。他们会把这些混在一起，因为它们在情境上是最好的。

### 00:31:51,92

![00-31-51](tmp/00-31-51.jpg)

好了，就这样了。就像先决条件信息一样，要真正能够为实际用例进行一些神经网络的训练和执行，那么让我们开始吧。

### 00:32:05,76

![00-32-05](tmp/00-32-05.jpg)

因此，首先讨论如何设置网络并对其进行训练，我们将在称为 Mnist 数据集的东西上训练网络，这是一堆手绘数字。

我在这张幻灯片上包含了一些，显示了一堆零。

这些图像都是 28 x 28 的，所以这意味着，如果我们想将这些图像插入到神经网络中，那就是 784 像素。

所以我们需要 784 个输入神经元。

除了输入层之外，我们还将有一个包含 30 个神经元的隐藏层。

然后，当我们评估网络时，我们将有一个包含 10 个神经元的输出层。

具有最大输出值的 neuron 输出将被视为获胜者，并且该索引将是网络已识别的值。

所以，你知道，如果 outlook 0 具有最大的值，那么我们会说网络将其识别为 0。

如果输出 5.最大的价值是什么？

然后我们假设网络引用 5，所以把所有层加起来，就是 824 个神经元。

正是将所有连接和偏差相加，才产生了近 24,000 个参数，我们正在努力优化，这是相当多的。

但就神经网络而言，这实际上是一个相当小的网络。

如果你能相信的话。好。这就是拓扑结构，有点像整体，我们要为训练 Ms 数据集的网络做 2 个数据子集。

有 60,000 位数字用于训练网络。

然后有 10,000 个用于在训练后测试网络，所以我们在这里做的是，我们使用 60,000 个进行训练，然后我们评估结果的质量。

使用这 10,000 个。我们这样做的原因是神经网络是另一种优化技术。

它们有时会过度拟合训练数据。这意味着他们非常擅长训练数据。

但他们不擅长其他任何事情。因此，如果你拿着你的训练数据，并保留一些东西来测试你正在做的事情，你就允许自己在以前从未见过的数据上测试网络，这将告诉你它是否结束，是否适合。

因此，对于我们的激活函数，我们将使用病态点，它本质上是一个可微分的阶跃函数，而不是我们目前一直在讨论的符号函数。

如果你知道脚趾映射，它可能看起来非常相似，这就是我们所做的，我们基本上是把所有可能的浮点数都映射到，你知道的，对于步骤类型的情况，它们是 0 到 1 的映射。

当我们进行训练时，我们将执行 30 个 epoch。

因此，我们将对这 60,000 个号码进行 30 次检查。

我们将对数字的顺序进行随机排序。

每个 epoch 中，我们将有一个 10 的小批量大小，这意味着当我们每 10 个项目运行这 60,000 个训练项时，我们将获得成本函数的平均梯度，然后我们将调整我们的权重和偏差。我们将使用 3 点 O 的训练学习率，这意味着我们将梯度乘以 3，然后再从权重和偏差中减去它。

然后，对于我们整个网络的成本函数，我们只需将 10 个输出神经元中的每一个的成本函数相加，单个输出神经元的成本函数将是所需输出的一半。

减去实际的输出平方，或者换句话说，它是误差平方的一半。

我们添加了 one half，因为当你取导数时，它会用平方抵消 2 的幂。所以这意味着你的导数实际上只是实际输出减去期望值，这很好，也很简单。

### 00:36:26,24

![00-36-26](tmp/00-36-26.jpg)

好的，我来给你展示这个东西的实际效果。

但首先还有几件事。我用 3 种梯度方法中的每一种来训练这个东西，我想谈谈我们所做的那些方法，这些方法最终在 10,000 张训练数据图像上获得了 95% 的准确率，从这个角度来看。

这意味着每 20 个就出局。它出错了，这是一个相当大的错误率。但这种就像玩具级的神经网络，所以结果相当不错，因为，就像一个玩具。

但是，是的，有一个更大的网络和卷积神经网络以及不同的激活函数。

这样可以为您带来比这更好的结果。因此，在有限差分的训练中，我们基本上必须评估函数。

24,000 次来获取每个输入的每个梯度，这有点多。

还有我。所以我使用 Omp 将其分散到 18 个课程中，并且还有一些其他优化。

而且还是花了 6 小时。是的。使用中心差分，这需要两倍于函数评估的数量，而一个函数评估需要 11 个半小时。

所以训练的时间相当长。这个非常小的神经网络，使用工具编号来代替来获得梯度，一旦我用天真的双数实现来尝试它，我就破坏了堆栈，因为每个变量的临时局部成本以及所有的东西，它里面有 24,000 个浮点数，所以我就像立即用完了空间。

所以我做了一个稀疏工具编号类，并编造了一个稀疏拉取的堆栈分配器，然后用 theiler 尽可能地打击它，但仍然只能把它减少到 28 个半小时，希望一天有点疯狂。是的，回来了。它只用了 90 秒。我也不需要做任何特别的事情来进行分析。

所以我认为这只是表明宏观优化可以比微观优化大得多。

你知道的，我们都知道。但是在 Gpu 上尝试这些东西并查看它们的特性是否发生了任何变化也很有趣。

### 00:38:56,72

![00-38-56](tmp/00-38-56.jpg)

所以，下一步我将展示演示。但我想快速展示一下我实际上是如何在演示中通过神经网络运行的，这是蓝色框所在的渲染图。

资源（如纹理和缓冲区）以及橙色框是计算着色器。

我已经圈出了 2 个计算着色器，它们是实际执行神经网络评估的着色器。

只是那 2 个并插入它们。我圈出的这个缓冲区是 93 kB 的长笛。

这些是由训练应用程序训练的权重和设备，只需将其加载到 GPU 的内存中，然后馈送到这些计算份额。其余的 mutute 着色器、纹理和缓冲区基本上都用于 Ui。

### 00:39:52,40

![00-39-52](tmp/00-39-52.jpg)

所以，是的，这里，这是在我们实际启动它之前的示例，我想在分析窗口中向您展示。

所以我们每秒总共使用 283 个麦克风。

因此，不到三分之一毫秒即可识别该数字。

这是在 30， 90 上。但是，是的，超级快。因此，让我实际启动演示并实时向您展示它。

### 00:40:18,64

![00-40-18](tmp/00-40-18.jpg)

### 00:40:22,40

![00-40-22](tmp/00-40-22.jpg)

好的，太好了。所以这是演示。你可以只用鼠标左键绘图，然后看到它识别 0。

一个 1，一个 2，当我清除它时，重要的是要注意它何时什么都没有得到。

如果认为它有一个 5.这很有趣，但如果你仔细想想，我们只用数字来训练网络。

我们从来没有给它任何无效的东西，也没有给它任何方法来表示某件事是无效的，所以它必须做出猜测。

所以 5 和任何数字一样好，我猜当我画一个数字时，这里显示的其他东西。

这里的第一个框显示了 784 个输入神经元。

安排他的像素。下一列框。

这是隐藏层，最后一列框是输出层。

是的，当你分析它时，你可以在这里看到隐藏层实际上花费的时间更少。

它看起来总共有 100 50 微秒，这真是超级快，超级超级快。

所以，是的，这就是演示。

### 00:41:38,16

![00-41-38](tmp/00-41-38.jpg)

### 00:41:40,96

![00-41-40](tmp/00-41-40.jpg)

您可以触发演示。它只是 c 加 dx 12。

没有外部库，所以你应该能够，只是，你知道的，打开它并编译它。

我在 Visual Studio 2022 中做到了，所以如果你使用的是 2019，我可能会抱怨。

但只要告诉它它可以转换。所以最后，希望你能看到这些东西并不那么可怕，就像反向传播可能是最相同的部分。

但这并没有那么糟糕，如果需要，您可以随时选择类似的选项。

您知道并只需支付更长的培训时间的成本。

但实际上，我想在这里展示的重要一点是。

此外，蒸汽很好。所有这些东西是，虽然训练神经网络和类似的东西可能真的很慢，但实际上在运行时使用它们可能会非常快，因为，你知道，这只是一些矩阵乘法，你知道的，那些非常快，尤其是在 Gpu 上，所以我觉得我想让你知道，你不应该害怕运行这些运行时的东西， 尽管训练需要很长时间，如果你想知道，有点像你喜欢的机器学习知识 One Way 的下一步去哪里。

接下来，你实际上可以修改 Kitl 存储库中的训练应用程序，你可以尝试为学习增加动力，或者，你知道的，去阅读 Adam 是如何工作的，并尝试实现它，你也可以尝试不同的激活函数，如 Relu

或 Sellu，特别是对于隐藏层，然后你也可以尝试使用卷积神经网络而不是我们所做的完全连接的感知器类型的网络，卷积神经网络往往在处理图像时做得更好，这正是 Ms 数据集，所以应该在那里工作得更好。你也可以尝试用不同的东西进行交易，比如，还有另一个训练集，比如 mnist，但它叫做 C far 10，它只是一些彩色图像，你可以训练一个网络。

喜欢。它们都有标签，如果你尝试并得到更好的结果或更差的结果，很高兴听到你的意见。

最后，还有一个叫做 onyx 运行时的东西，我们有一个 Ea 包装器。

这基本上是帮助你运行推理，或者更确切地说，在 Runtime 更轻松、更高效地执行你的神经网络。

所以我们确实有一个合适的说唱者。

它被用于 fea 等游戏。

但它并不是特定于冻伤的。所以如果你不使用 frostbite，你就会对此感兴趣。

这是你可以做的。你可以使用，这很酷，最后，我想说，你知道，我们 EA 有很多游戏开发专家，我们也有很多机器学习专家。

但是两者之间没有太多重叠，因为就像一个是裸机 c plus plus 和 Gpu 优化。

对于 game def，另一个就像在 Python 世界中工作。

你知道高等数学和研究论文，没有一本书有很多重叠，但我真的觉得，如果我们能从两方面帮助弥合我们两个世界之间的差距，我们真的可以做一些了不起的事情。

所以我认为，机器学习 Sig 是一个很好的地方。所以希望我们能在那里进行一些对话，并尝试建立一座桥梁，并做一些非常酷的 Next Gen. Stuff。

就这样。非常感谢你们的聆听。

打我。如果您有任何问题。

### 00:45:38,16

![00-45-38](tmp/00-45-38.jpg)
