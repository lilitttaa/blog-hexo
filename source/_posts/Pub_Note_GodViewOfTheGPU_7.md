---
title: 上帝视角看GPU（7）：虚拟GPU
---

一般提到虚拟GPU的时候，都只是个狭义的定义。这里打算扩展一下，讲讲广义的虚拟GPU技术。


我们先从需求开始：

![alt text](image.png)

首先复习一下GPU的软硬栈。从简化的分层来看，最上层是应用，它调用API，来到驱动，最后到达GPU硬件。

至于虚拟GPU，自然也是可以在不同的层面完成。所以这还是个切一刀的问题，需要根据需求切在不同地方。


![alt text](image-1.png)
第一个也是最古老的需求，来自于人民群众喜闻乐见的游戏机模拟器。

古老的NES模拟器，就需要模拟出PPU这个图像处理器，从指令集的层面上做到尽可能一致的行为。

也就是用纯软件模拟个硬件，用来生成图像。

![alt text](image-2.png)

对于封闭架构的游戏机平台来说，第三方往往得在硬件层面进行模拟。

因为整个软件栈很薄，而且包含在游戏的ROM里，外界无从知晓细节，没法从中间切开。

而开放架构的模拟器，就不必到那么底层，在开放的地方切就行。


![alt text](image-3.png)
比如Android的模拟器，就只需要在驱动层面进行模拟。

又因为Android里图形API层和图形驱动层是合一的，所以这相当于API层的模拟。

![alt text](image-4.png)
把虚拟机里app和系统的图形API调用转到host执行，而host可以是其他系统、其他API。


![alt text](image-5.png)
另一个典型就是Windows里的HyperV虚拟机。

这个简单一些，host和VM里的系统和API一样，虚拟GPU是在驱动层进行模拟。

在VM里的应用来看，就是个普通的GPU。

而虚拟GPU的驱动，把数据流发给host的服务，服务再调用host的API，在真的GPU上执行。

![alt text](image-6.png)
它们都是用一个GPU模拟出另一个GPU。

第二个需求，是为了更高的渲染速度。

![alt text](image-7.png)
由于图形渲染本质上有着天生的并行性，很容易可以对渲染区域做一些划分，分配到不同的GPU上执行。


![alt text](image-8.png)
这就有了NVIDIA的SLI和AMD的CrossFire这样的技术。

它们在硬件和驱动上做了分配的工作，把多个GPU虚拟成一个超强的GPU来使用。

![alt text](image-9.png)
要么把一帧的不同部分分配给不同的GPU，同时渲染，让每一帧渲染的速度翻倍。

![alt text](image-10.png)
要么奇数帧在一个GPU上渲染，偶数帧在另一个，同时渲染两帧。

![alt text](image-11.png)
在硬件上SLI需要一个连接片，让两个GPU互相传送指令和数据

![alt text](image-12.png)
在驱动层需要做一定的任务分配。

再往上的API层和应用层，并不需要考虑这件事情的存在，当作一个GPU用就是了。

![alt text](image-13.png)
除了在一帧之内划分，以及帧间划分之外，还有一个维度，就是对一条流水线的不同阶段划分。

现代的实时渲染，几乎都有3D渲染和后处理两步。

后处理的输入和输出都是2D图像，计算量也没那么大。

![alt text](image-14.png)
比如在游戏设置里经常可以看到的HDR tone mapping，AA，这些都是后处理做的。

![alt text](image-15.png)
在D3D12/Vulkan时代，多个GPU可以通过映射显存，在一定程度上相互访问。

这就使得可以在应用层，手工进行不同品牌不同型号的多GPU混合渲染，更加灵活。


![alt text](image-16.png)
如果显示器是接在集成显卡上，那么集成显卡就是渲染和输出设备，独立显卡只是渲染设备。

即便切换成独立显卡来渲染，最后也要经过集成显卡进行输出。


![alt text](image-17.png)
如果这里用上了多GPU混合渲染，就能在独立显卡上进行渲染，得到的图片映射给集成显卡。

集成显卡接着做后处理的步骤，最后输出给显示器。

与此同时独立显卡已经开始渲染下一帧了。

这样性能略优于只用独立显卡的。

![alt text](image-18.png)
对于负荷不重的任务，也可以直接在集成显卡上进行，降低功耗。

第三个需求，也和任务划分有关，却是来自于方便计算任务的开发。

![alt text](image-19.png)
不同的GPU，结构有所不同，要发挥出最大效率，往往需要设置不同的参数。

要是针对每一款GPU，都单独做这些细节调整，开发会变得旷日持久。

所以这种虚拟GPU的方法，就是通过编译器和库进行抽象，在应用层把GPU的细节隐藏起来，甚至把有几个GPU也隐藏起来。

用户只管组织一个大任务，由编译器和库把它自动拆分，送去GPU。

机器学习领域常用的PyTorch和Tensorflow就是代表。

把数据存成tensor的形式，在库里面实现好tensor的各种操作。

开发的时候只要使用tensor就可以了，不需要考虑tensor的计算，具体是如何映射到哪个GPU的哪个线程的。

更进一步，这样通过抽象来虚拟GPU的方式，还能把多个GPU和CPU都统一到同样的接口上。

这就更加通用了，甚至可以把任务同时分配给CPU和GPU，异构执行。

第四个需求，是现阶段很常见的情况，那就是GPU共享。

这种时候需要把一个GPU映射成多个，给不同的用户使用。

在每个用户看来，自己都是独占使用这个GPU的。

这就是狭义的虚拟GPU。

每个用户不会总是占着100%的GPU，所以通过共享就提高了利用率，降低成本。

通过在云上提供GPU服务，现在很多计算都是在那上面跑的。

用户并不需要知道自己的程序具体在哪个GPU上跑，又跟谁共享了。

GPU的计算资源成了像水和电一样，根据使用量计费的东西。

这种虚拟一般需要在驱动层做，把虚拟机的GPU调用转到host的物理GPU。

原理上看，这其实和前面提到的HyperV的虚拟GPU很像。

不同的是，这里的需求是支持多个虚拟机，对效率要求较高，所以host端是直接接入驱动的。

另外，多个虚拟机之间有隔离的要求。

深入来说，在驱动层会给每个用户会话建立一个context，每个context只能访问自己的资源。

这样多个context就能共享一个硬件又不能互相访问。

但这样的隔离程度有时候还不够。

比如，常见的是一个虚拟GPU刚释放的显存，内容没有清掉就又被分配给了另一个虚拟GPU，也就会被另一个用户访问到。

现在有的GPU有额外的防护，在硬件层面做隔离。

通过类似于CPU上常用的页表和进程权限控制，即便显存指针越界也不会访问到其他虚拟GPU的空间。

这几种不同类型的GPU虚拟方式，并不具有排他性。

它们经常是叠加来使用的。

其中最常见的，是4和3的叠加，先把一个GPU模拟成多个，又在模拟出来的一个上跑机器学习的任务。

这就是当前云GPU服务大部分时候在做的事情。

云游戏则可以看作4和1的叠加。

把一个GPU模拟成多个用于渲染的GPU，每一个可能模拟成了不同型号的GPU。

这几种就是常见的虚拟GPU分类。

下一期来看看不一样的主题。欢迎继续收看，再见。
